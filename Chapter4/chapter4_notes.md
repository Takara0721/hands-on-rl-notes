## 策略迭代

虽然笔者在上章节说过状态价值函数可以通过贝尔曼期望方程$$\mathcal{V}_{k+1} = \mathcal{R} + \gamma P \mathcal{V}_{k}$$

进行迭代得到，即策略评估，但实际上在开发中，我们一般使用$$V^{k+1}(s) = \sum_{a \in A} \pi(a|s) \left( r(s, a) + \gamma \sum_{s' \in S} p(s'|s, a) V^{k}(s') \right)$$

对$s$进行遍历，最后获得$\mathcal{V}_{k+1}$，也就是说有两种方式来进行策略评估

- 通过MDP构建MRP，再使用矩阵迭代
    - 这种方法在状态空间不大时可以使用，在状态空间比较大时，需要非常消耗内存
- 直接使用MDP进行迭代
    - 这种方法对会比矩阵迭代消耗的内存空间少，当状态转移是稀疏的时候，往往更有效率

关于贝尔曼期望方程，我们在$Chapter2$中，讨论的是环境的奖励机制是确定性的，如果我们的环境的奖励机制是随机的话，如书中冰壶环境

那么对应的，我们的贝尔曼期望方程应该也要改写为$$V^{\pi}(s) = \sum_{a \in A} \pi(a|s) \left( \sum_{s' \in S} P(s'|s, a) [r(s, a, s') + \gamma V^{\pi}(s')] \right)$$

$$Q^{\pi}(s, a) = \sum_{s' \in S} P(s'|s, a) \left[ r(s, a, s') + \gamma \sum_{a' \in A} \pi(a'|s') Q^{\pi}(s', a') \right]$$

## 价值迭代

在价值迭代中，同样地，参考策略迭代，如果我们环境的奖励机制是随机的话，对应的我们贝尔曼最优方程应该修改成$$V^*(s) = \max_{a \in A} \{\sum_{s' \in S} P(s'|s, a) [ r(s, a, s') + \gamma V^{\pi}(s') ] \} $$

价值迭代收敛性证明中用到了压缩映射定理，笔者这里给出压缩映射定理的简短介绍

[压缩映射定理介绍](https://blog.csdn.net/weixin_48956550/article/details/137146801)
